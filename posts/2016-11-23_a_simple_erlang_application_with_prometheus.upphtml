<!--
A Simple Erlang Application, with Prometheus
November 23, 2016
erlang, prometheus
-->
    <section>
    
        <p>A simple HTTP server built with Elli that exposes metrics for Prometheus.
    
    	<figure>
        <img src='/img/full/fire.png' alt='Face with a fiery torch in the background.' class='webfeedsFeaturedVisual'/>
    	<figcaption>
           &copy; 2016 <a href='/imginfo/fire.html'>Vinicius Amano</a>
    	  for Unsplash
    	</figcaption>
    	</figure>
    
    </section>
    <section>
    <h2>The Code</h2>
    
    <p>The code is identical to the code in <a href="2016-11-09_a_simple_erlang_application.html">A Simple Erlang Application</a>, with the following modifications:
    
    <ul>
    <li>new dependency on an <a href="https://github.com/deadtrickster/prometheus.erl">Erlang client</a> for <a href="https://prometheus.io/">Prometheus.io</a>, a monitoring system and time series database,</li>
    <li>new handler to log metrics and expose an endpoint for Prometheus, and</li>
    <li>new child spec to call elli_middleware and stack the prometheus.io handler before the original handler.</li>
    </ul>
    
    <p>You can find the code for this blog entry <a href="https://github.com/mbucc/markbucciarelli.com/tree/master/sandbox/prometheus">here</a>.
    
    <h3>New dependency on Erlang client for Prometheus.io</h3>
            <pre><code>
    PROMETHEUS_GIT=https://github.com/deadtrickster/prometheus.erl.git
    PROMETHEUS_TAG=v3.1.0
    PROMETHEUS=${SRC}/prometheus.erl
    PROMETHEUS_APP=${PROMETHEUS}/_build/default/lib/prometheus
    
    ${PROMETHEUS}:
    	(cd ${SRC}; git clone ${PROMETHEUS_GIT})
    	(cd ${PROMETHEUS}; git checkout tags/${PROMETHEUS_TAG})
    ${PROMETHEUS_APP}/ebin: ${PROMETHEUS}
    	(cd ${PROMETHEUS} ; rebar3 compile)
    </code></pre>
    
    <p>This new section in the Makefile downloads and compiles the Erlang client for prometheus.  The <code>${PROMETHEUS_APP}/ebin</code> is then added as a dependency for compiling the Erlang code in this project.
    
    
    <h3>New handler to log metrics and expose an endpoint for Prometheus</h3>
    
    
            <pre><code>
    %% @doc Exposes HTTP responses and timings as Prometheus metrics.
    %%
    %% Defines two metrics:
    %%
    %%        * the counter http_requests_total, with the label statuscode.
    %%
    %%        * the histogram http_response_microseconds
    %%
    %% The duration measures the time, in microseconds, between when
    %% the request line was received and the response was sent. This
    %% is as close we can get to the actual time of the request as
    %%  seen by the user.
    
    -module(elli_prometheus).
    
    -behaviour(elli_handler).
    
    -include_lib("elli/include/elli.hrl").
    
    -export([handle/2, handle_event/3]).
    
    % Expose /metrics for Prometheus to pull.
    handle(Req, _Args) ->
        handle(Req#req.method, elli_request:path(Req), Req).
    
    handle(&rsquo;GET&rsquo;, [<<"metrics">>], _Req) ->
        {ok, [], prometheus_text_format:format()};
    % All other requests are passed to normal handler.
    handle(_Verb, _Path, _Req) -> ignore.
    
    % Return path, minus any query string, as a binary.
    rawpath(#req{raw_path = Path}) ->
        case binary:split(Path, [<<"?">>]) of
          [P, _] -> P;
          [P] -> P
        end.
    
    
    handle_event(request_complete,
                 [Req, ResponseCode, _ResponseHeaders, _ResponseBody,
                  Timings],
                 _Config) ->
        prometheus_counter:inc(http_requests_total,
                               [ResponseCode, rawpath(Req)]),
        Start = proplists:get_value(request_start, Timings),
        End = proplists:get_value(request_end, Timings),
        Delta = timer:now_diff(End, Start),
        io:format("handle_event: ~p  ~p  ~p~n",
                  [ResponseCode,
                   erlang:convert_time_unit(Delta, native,
                                            micro_seconds),
                   rawpath(Req)]),
        % The "_microseconds" suffix in the metric name is magic.
        % Prometheus.erl converts the Erlang native time difference to microseconds.
        prometheus_histogram:observe(response_time_in_microseconds,
                                     [rawpath(Req)], Delta),
        ok;
    handle_event(chunk_complete,
                 [Req, ResponseCode, ResponseHeaders, _ClosingEnd, Timings],
                 Config) ->
        handle_event(request_complete,
                     [Req, ResponseCode, ResponseHeaders, <<>>, Timings],
                     Config);
    handle_event(Event, [Req, _Exc, _Stack], _Config)
        when Event =:= request_throw;
             Event =:= request_exit;
             Event =:= request_error;
             Event =:= request_parse_error;
             Event =:= bad_request;
             Event =:= client_closed;
             Event =:= client_timeout ->
        prometheus_counter:inc(http_requests_total,
                               [Event, rawpath(Req)]),
        ok;
    handle_event(elli_startup, [], _Config) ->
        prometheus_counter:new([{name, http_requests_total},
                                {help, "Total HTTP requests"},
                                {labels, [status, path]}]),
        prometheus_histogram:new([{name,
                                   response_time_in_microseconds},
                                  {labels, [path]},
                                  {buckets,
                                   [10, 100, 1000, 10000, 100000, 300000, 500000,
                                    750000, 1000000, 1500000, 2000000, 3000000]},
                                  {help,
                                   "Microseconds between request receipt "
                                   "and response send."}]),
        ok;
    handle_event(_, _, _) ->
        %% Future-proof.
        ok.
    </code></pre>
    
    <p>Lots of code here.  Some things of note:
    
    <ol>
    <li>This is a normal elli callback module, just like <code>es_callback.erl</code> in the <a href="2016-11-09_a_simple_erlang_application.html">last blog entry</a>.  As shown in the next section, the <code>es_sup</code> module stacks the handlers when configuring <code>elli_middleware</code>.</li>
    <li>If a callback module returns the atom <code>ignore</code>, the request is passed to the next handler in the stack.</li>
    <li>The <code>elli_startup</code> event (the last <code>handle_event</code> in the listing above) is where you define the metrics.</li>
    <li>A metric label let&rsquo;s you create subtotals.  For example, <code>http_requests_total</code> has subtotals by status and resource.</li>
    <li>The <code>_microseconds</code> suffix on the metric name is &ldquo;magic&rdquo;; that is, it tells the client library to convert native time to microseconds.  See full list of magic suffixes at <a href="https://github.com/deadtrickster/prometheus.erl/blob/6dd56bf321e99688108bb976283a80e4d82b3d30/src/prometheus_time.erl#L27-L37">lines 27 - 37</a> of <code>prometheus_time.erl</code></li>
    <li>This handler exposes the <code>/metrics</code> resource, which is the default endpoint that Prometheus looks for.</li>
    <li>This handler is decoupled from all other handlers.</li>
    </ol>
    
    <p>I figured out this code by looking at other elli middleware (for example, <a href="https://github.com/wooga/elli_access_log/blob/master/src/elli_access_log.erl">access logging</a>) and then finding the elli-lib/elli_prometheus <a href="https://github.com/elli-lib/elli_prometheus/blob/master/src/elli_prometheus.erl">example</a>.
    
    <h3>New child spec to call elli_middleware</h3>
    
            <pre><code>
    -module(es_sup).
    
    -behaviour(supervisor).
    
    -export([start_link/0]).
    
    -export([init/1]).
    
    start_link() ->
        supervisor:start_link({local, ?MODULE}, ?MODULE, []).
    
    init([]) ->
        MiddlewareConfig = [{mods,
                             [{elli_prometheus, []}, {es_callback, []}]}],
        ElliOpts = [{callback, elli_middleware},
                    {callback_args, MiddlewareConfig}, {port, 3000}],
        ElliSpec = {es_http, {elli, start_link, [ElliOpts]},
                    permanent, 5000, worker, [elli]},
        {ok, {{one_for_one, 5, 10}, [ElliSpec]}}.
    </code></pre>
    
    <p>The difference from the last blog post is the <code>MiddlewareConfig</code>, which stacks the handlers and <code>ElliOpts</code>, which tells Elli to use the middleware (which itself is a handler).
    
    <h3>prometheus.yml</h3>
    
            <pre><code>
    global:
      scrape_interval:     1s
    
      external_labels:
        monitor: &rsquo;elli-monitor&rsquo;
    
    scrape_configs:
      - job_name: &rsquo;prometheus&rsquo;
        static_configs:
          - targets: [&rsquo;localhost:9090&rsquo;]
    
      - job_name: &rsquo;erlsrv&rsquo;
        static_configs:
          - targets: [&rsquo;127.0.0.1:3000&rsquo;]
    </code></pre>
    
    
    <p>This configures prometheus.  Change to directory that holds this file and start prometheus.
    
    
    </section>
    <section>
    
    <h2>Notes</h2>
    
    <h3>elli: version 1.0.5 versus version 2.0.0</h3>
    
    <p>The elli_lib project on GitHub has released a 2.0.0 version of elli with these changes (among others):
    <ol>
    <li>tracks response size as well as timings</li>
    <li>uses Erlang&rsquo;s monotonic clock</li>
    <li>sprinkles lots of <code>spec</code> definitions throughout the code base</li>
    <li>changes the semantics of request_start event</li>
    </ol>
    
    <p>The last change was giving me request durations that were over second seconds in some cases, which is why I stayed with version 1.0.5.
    
    
    <h3>Built in metrics that come with prometheus.erl</h3>
    
    <p>By simply including the <code>prometheus.erl</code> client, I automatically get a ton of metrics exposed on the <code>/metrics</code> endpoint, including:
    
    <ul>
    <li>Total number of context switches since the system started</li>
    <li>Garbage collection: number of GCs</li>
    <li>Garbage collection: bytes reclaimed</li>
    <li>Total length of the run-queues</li>
    <li>The total amount of memory currently allocated.</li>
    <li>&hellip; and so on.</li>
    </ul>
    
    <p>You can find the entire list in the README for <a href="https://github.com/deadtrickster/prometheus.erl">https://github.com/deadtrickster/prometheus.erl</a> in the section &ldquo;Erlang VM & OTP Collectors.&rdquo;
    
    </section>
    
    <section>
    <h2>Prometheus Memory Consumption</h2>
    
    <p>I plan to run Prometheus on a Raspberry Pi 3, which only has 1G of RAM.  Some notes on prometheus memory usage:
    
    <ol>
    <li>stores metric data in &ldquo;chunks&rdquo;</li>
    <li>each chunk is 1024 bytes</li>
    <li>keeps all &ldquo;currently used&rdquo; chunks in memory</li>
    <li>limit on in-memory chunks set by <code>storage.local.memory-chunks</code> config, default value is 1048576 bytes (1 MB)</li>
    <li>don&rsquo;t hit limit, as metric data is dropped (&ldquo;throttled ingestion&rdquo;)</li>
    <li><code>storage.local.retention</code> config defines how long to store metric history on disk</li>
    </ol>
    
    <p>Rule of thumb:
    <ul>
    <li>prometheus RAM required = 3 x <code>storage.local.memory-chunks</code> x 1024 bytes</li>
    <li>plan for at least three active chunks for each time series you have</li>
    <li>monitor prometheus memory usage with two metrics exported by prometheus itself:
    <ol>
    <li><code>prometheus_local_storage_memory_chunks</code></li>
    <li><code>process_resident_memory_bytes</code></li>
    </ol>
    </li>
    </ul>
    
    <p>Reference: <a href="https://prometheus.io/docs/operating/storage/">https://prometheus.io/docs/operating/storage/</a>.
    </section>
    
    <section>
    <h2>Results</h2>
    
    <p>Starting up the application
    
            <pre><code>
    $ ./run.sh
    Erlang/OTP 19 [erts-8.0.1] [source-ca40008] [64-bit] [smp:8:8] [async-threads:10] [hipe] [kernel-poll:false]
    
    Eshell V8.0.1  (abort with ^G)
    1> application:start(erlsrv).
    ok
    2> 
    </code></pre>
    
    <p>and hitting it with some traffic
    
            <pre><code>
    #! /bin/sh -e
    # Lay siege to Elli.
    
    CLIENTS=5
    DURATION=60S
    siege --log=siege.log -c $CLIENTS -t $DURATION http://127.0.0.1:3000/hello/world
    
    CLIENTS=50
    DURATION=60S
    siege --log=siege.log -c $CLIENTS -t $DURATION http://127.0.0.1:3000/hello/world
    
    CLIENTS=5
    DURATION=60S
    siege --log=siege.log -c $CLIENTS -t $DURATION http://127.0.0.1:3000/hello/world
    </code></pre>
    
    <p>loads Prometheus with some data.  Here are a examples of Prometheus graphs.
    
    <h3>http_requests_total</h3>
    <p>
    <figure class="fullwidth">
    <img src="/img/prometheus_http_requests.png"  alt="Graph of http requests against time looks sigmoid-ish" />
    </figure>
    <p>Prometheus graphs metrics by label and job.  I chose the same metric name as prometheus uses internally, so there is a name collision; prometheus uses different labels than I did, so the first four rows in the legend have the labels code, handler, instance, and method (the Prometheus labels), and the last three have instance, job and path (the erlsrv labels).
    
    <h3>rate(response_time_in_microseconds_sum[15s]) / rate(response_time_in_microseconds_count[15s])</h3>
    <p>Prometheus supports expressions; this one computes average response time over last 15 seconds.
    <figure class="fullwidth">
    <img src="/img/prometheus_response_time.png" />
    </figure>
    <p>The <code>/metrics</code> page hovers under six microseconds, and <code>/hello/world</code> is well under one microsecond.
    
    <h3>erlang_vm_memory_bytes_total</h3>
    <p>
    <figure class="fullwidth">
    <img src="/img/prometheus_erlang_vm_memory.png" />
    </figure>
    
    <p>System memory up over 16M, process memory just over 4M, both stable.  Not surprisingly, the &ldquo;load&rdquo; I put on (200 requests/second for a tiny response body with no backend work) doesn&rsquo;t register.
    
    <h3>prometheus_local_storage_memory_chunks</h3>
    <p>
    <figure class="fullwidth">
    <img src="/img/prometheus_chunks.png" />
    </figure>
    
    <p>This one kept climbing to the right of this graph after I shut off erlsrv, until it plateaued at 7.5K.
    
    
    </section>
